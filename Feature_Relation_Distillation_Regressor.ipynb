{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyIStS2pLV9N",
        "outputId": "e920e954-5685-480c-e0f4-4bd38aabe705"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26.4M/26.4M [00:02<00:00, 13.1MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29.5k/29.5k [00:00<00:00, 208kB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.42M/4.42M [00:01<00:00, 3.89MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.15k/5.15k [00:00<00:00, 23.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.45593280092612515\n",
            "Epoch 2/10, Loss: 0.2588799115913763\n",
            "Epoch 3/10, Loss: 0.2103206562335049\n",
            "Epoch 4/10, Loss: 0.18105123428774794\n",
            "Epoch 5/10, Loss: 0.1549719602171419\n",
            "Epoch 6/10, Loss: 0.1327816787352567\n",
            "Epoch 7/10, Loss: 0.11191160735418039\n",
            "Epoch 8/10, Loss: 0.0959089513637745\n",
            "Epoch 9/10, Loss: 0.07870940383134493\n",
            "Epoch 10/10, Loss: 0.06781428760445829\n",
            "Test Accuracy: 92.92%\n",
            "Norm of 1st layer of nn_light: 2.3761112689971924\n",
            "Norm of 1st layer of new_nn_light: 2.3761112689971924\n",
            "DeepNN parameters: 938,922\n",
            "LightNN parameters: 206,010\n",
            "Epoch 1/10, Loss: 0.5243566697086098\n",
            "Epoch 2/10, Loss: 0.3378505093265952\n",
            "Epoch 3/10, Loss: 0.2906147592357481\n",
            "Epoch 4/10, Loss: 0.2623042359725753\n",
            "Epoch 5/10, Loss: 0.242506469220622\n",
            "Epoch 6/10, Loss: 0.22283650443815728\n",
            "Epoch 7/10, Loss: 0.20786556247264337\n",
            "Epoch 8/10, Loss: 0.19598372306015446\n",
            "Epoch 9/10, Loss: 0.18320772083583417\n",
            "Epoch 10/10, Loss: 0.1710275296987628\n",
            "Test Accuracy: 91.25%\n",
            "Teacher accuracy: 92.92%\n",
            "Student accuracy: 91.25%\n",
            "Epoch 1/10, Loss: 0.8409164476750502\n",
            "Epoch 2/10, Loss: 0.5031007178810868\n",
            "Epoch 3/10, Loss: 0.42338366383936865\n",
            "Epoch 4/10, Loss: 0.3744208828282\n",
            "Epoch 5/10, Loss: 0.34146436705772304\n",
            "Epoch 6/10, Loss: 0.3143707987532687\n",
            "Epoch 7/10, Loss: 0.2848170559638853\n",
            "Epoch 8/10, Loss: 0.26866103165439453\n",
            "Epoch 9/10, Loss: 0.24875427758706403\n",
            "Epoch 10/10, Loss: 0.2336714507769674\n",
            "Test Accuracy: 91.46%\n",
            "Teacher accuracy: 92.92%\n",
            "Student accuracy without teacher: 91.25%\n",
            "Student accuracy with CE + KD: 91.46%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "# Check device (supports torch.accelerator if available, else CPU)\n",
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Preprocessing for FashionMNIST (grayscale, so mean and std are single values)\n",
        "transforms_mnist = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.2860], std=[0.3530]),  # typical mean/std for FashionMNIST\n",
        "])\n",
        "\n",
        "# Loading FashionMNIST dataset:\n",
        "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms_mnist)\n",
        "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transforms_mnist)\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "# Adjusted DeepNN and LightNN for single channel input (1 channel instead of 3)\n",
        "\n",
        "class DeepNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(DeepNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 128, kernel_size=3, padding=1),  # input channel 1\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(32 * 7 * 7, 512),  # 28x28 images downsampled twice by factor 2 -> 7x7\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "class LightNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(LightNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1),  # input channel 1\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(16 * 7 * 7, 256),  # similarly 7x7 size after pooling\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Training and testing functions remain unchanged\n",
        "\n",
        "def train(model, train_loader, epochs, learning_rate, device):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "def test(model, test_loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Initialize and train teacher (DeepNN)\n",
        "nn_deep = DeepNN(num_classes=10).to(device)\n",
        "train(nn_deep, train_loader, epochs=10, learning_rate=0.001, device=device)\n",
        "test_accuracy_deep = test(nn_deep, test_loader, device)\n",
        "\n",
        "# Initialize student (LightNN)\n",
        "torch.manual_seed(42)\n",
        "nn_light = LightNN(num_classes=10).to(device)\n",
        "\n",
        "# Print norm of first layer weights for sanity check\n",
        "torch.manual_seed(42)\n",
        "new_nn_light = LightNN(num_classes=10).to(device)\n",
        "print(\"Norm of 1st layer of nn_light:\", torch.norm(nn_light.features[0].weight).item())\n",
        "print(\"Norm of 1st layer of new_nn_light:\", torch.norm(new_nn_light.features[0].weight).item())\n",
        "\n",
        "# Print total parameters\n",
        "total_params_deep = \"{:,}\".format(sum(p.numel() for p in nn_deep.parameters()))\n",
        "print(f\"DeepNN parameters: {total_params_deep}\")\n",
        "total_params_light = \"{:,}\".format(sum(p.numel() for p in nn_light.parameters()))\n",
        "print(f\"LightNN parameters: {total_params_light}\")\n",
        "\n",
        "# Train student\n",
        "train(nn_light, train_loader, epochs=10, learning_rate=0.001, device=device)\n",
        "test_accuracy_light_ce = test(nn_light, test_loader, device)\n",
        "\n",
        "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
        "print(f\"Student accuracy: {test_accuracy_light_ce:.2f}%\")\n",
        "\n",
        "# Knowledge Distillation training function remains unchanged\n",
        "\n",
        "def train_knowledge_distillation(teacher, student, train_loader, epochs, learning_rate, T, soft_target_loss_weight, ce_loss_weight, device):\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
        "\n",
        "    teacher.eval()\n",
        "    student.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                teacher_logits = teacher(inputs)\n",
        "\n",
        "            student_logits = student(inputs)\n",
        "\n",
        "            soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n",
        "            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
        "\n",
        "            soft_targets_loss = torch.sum(soft_targets * (soft_targets.log() - soft_prob)) / soft_prob.size()[0] * (T**2)\n",
        "            label_loss = ce_loss(student_logits, labels)\n",
        "\n",
        "            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "# Run knowledge distillation training\n",
        "\n",
        "train_knowledge_distillation(\n",
        "    teacher=nn_deep,\n",
        "    student=new_nn_light,\n",
        "    train_loader=train_loader,\n",
        "    epochs=10,\n",
        "    learning_rate=0.001,\n",
        "    T=2,\n",
        "    soft_target_loss_weight=0.25,\n",
        "    ce_loss_weight=0.75,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "test_accuracy_light_ce_and_kd = test(new_nn_light, test_loader, device)\n",
        "\n",
        "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
        "print(f\"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%\")\n",
        "print(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the code for the Feature Based distillation using intermediate layers using a regressor and then mse loss we can also use cosine loss here we are making the dimensionalities of the student intermediate layer equal to those of the teacher layer using a convolution layer in the regressor function used in the forward block of student model"
      ],
      "metadata": {
        "id": "pt730Orp2i4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have some doubt regarding the effectiveness or advantage of regressor\n",
        "\n",
        "I request you to please give some material about this topic"
      ],
      "metadata": {
        "id": "-DOk5X0c3BI8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sttA4cBDzzd"
      },
      "outputs": [],
      "source": [
        "# Create a sample input tensor\n",
        "sample_input = torch.randn(128, 1, 28, 28).to(device)\n",
        "\n",
        "# Pass the input through the student\n",
        "# logits, hidden_representation = modified_nn_light(sample_input)\n",
        "\n",
        "# # Print the shapes of the tensors\n",
        "# print(\"Student logits shape:\", logits.shape) # batch_size x total_classes\n",
        "# print(\"Student hidden representation shape:\", hidden_representation.shape) # batch_size x hidden_representation_size\n",
        "\n",
        "# # Pass the input through the teacher\n",
        "# logits, hidden_representation = modified_nn_deep(sample_input)\n",
        "\n",
        "# # Print the shapes of the tensors\n",
        "# print(\"Teacher logits shape:\", logits.shape) # batch_size x total_classes\n",
        "# print(\"Teacher hidden representation shape:\", hidden_representation.shape) # batch_size x hidden_representation_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWS92k_pLdcx",
        "outputId": "d1caf853-46a9-42fc-ead0-f06a11a535cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student's feature extractor output shape:  torch.Size([128, 16, 7, 7])\n",
            "Teacher's feature extractor output shape:  torch.Size([128, 32, 7, 7])\n"
          ]
        }
      ],
      "source": [
        "# Pass the sample input only from the convolutional feature extractor\n",
        "convolutional_fe_output_student = nn_light.features(sample_input)\n",
        "convolutional_fe_output_teacher = nn_deep.features(sample_input)\n",
        "\n",
        "# Print their shapes\n",
        "print(\"Student's feature extractor output shape: \", convolutional_fe_output_student.shape)\n",
        "print(\"Teacher's feature extractor output shape: \", convolutional_fe_output_teacher.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_2DNIUsLl6Q"
      },
      "outputs": [],
      "source": [
        "class ModifiedDeepNNRegressor(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ModifiedDeepNNRegressor, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(32 * 7 * 7, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        conv_feature_map = x\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x, conv_feature_map\n",
        "\n",
        "class ModifiedLightNNRegressor(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ModifiedLightNNRegressor, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        # Include an extra regressor (in our case linear)\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(16 * 7 * 7, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        regressor_output = self.regressor(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x, regressor_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWTExfI8Ll2y",
        "outputId": "4ddde218-697d-494d-a51f-6833ad38ef4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.6464882302385911\n",
            "Epoch 2/10, Loss: 0.4519972195630389\n",
            "Epoch 3/10, Loss: 0.4025828486947871\n",
            "Epoch 4/10, Loss: 0.37031873355287986\n",
            "Epoch 5/10, Loss: 0.34820735702382477\n",
            "Epoch 6/10, Loss: 0.3313671725391071\n",
            "Epoch 7/10, Loss: 0.3170622596735639\n",
            "Epoch 8/10, Loss: 0.3026722513599945\n",
            "Epoch 9/10, Loss: 0.29128484631270996\n",
            "Epoch 10/10, Loss: 0.2833304999987962\n",
            "Test Accuracy: 91.28%\n"
          ]
        }
      ],
      "source": [
        "def train_mse_loss(teacher, student, train_loader, epochs, learning_rate, feature_map_weight, ce_loss_weight, device):\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "    mse_loss = nn.MSELoss()\n",
        "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
        "\n",
        "    teacher.to(device)\n",
        "    student.to(device)\n",
        "    teacher.eval()  # Teacher set to evaluation mode\n",
        "    student.train() # Student to train mode\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Again ignore teacher logits\n",
        "            with torch.no_grad():\n",
        "                _, teacher_feature_map = teacher(inputs)\n",
        "\n",
        "            # Forward pass with the student model\n",
        "            student_logits, regressor_feature_map = student(inputs)\n",
        "\n",
        "            # Calculate the loss\n",
        "            hidden_rep_loss = mse_loss(regressor_feature_map, teacher_feature_map)\n",
        "\n",
        "            # Calculate the true label loss\n",
        "            label_loss = ce_loss(student_logits, labels)\n",
        "\n",
        "            # Weighted sum of the two losses\n",
        "            loss = feature_map_weight * hidden_rep_loss + ce_loss_weight * label_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "# Notice how our test function remains the same here with the one we used in our previous case. We only care about the actual outputs because we measure accuracy.\n",
        "\n",
        "def test_multiple_outputs(model, test_loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs, _ = model(inputs) # Disregard the second tensor of the tuple\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy\n",
        "\n",
        "# Initialize a ModifiedLightNNRegressor\n",
        "torch.manual_seed(42)\n",
        "modified_nn_light_reg = ModifiedLightNNRegressor(num_classes=10).to(device)\n",
        "\n",
        "# We do not have to train the modified deep network from scratch of course, we just load its weights from the trained instance\n",
        "modified_nn_deep_reg = ModifiedDeepNNRegressor(num_classes=10).to(device)\n",
        "modified_nn_deep_reg.load_state_dict(nn_deep.state_dict())\n",
        "\n",
        "# Train and test once again\n",
        "train_mse_loss(teacher=modified_nn_deep_reg, student=modified_nn_light_reg, train_loader=train_loader, epochs=10, learning_rate=0.001, feature_map_weight=0.25, ce_loss_weight=0.75, device=device)\n",
        "test_accuracy_light_ce_and_mse_loss = test_multiple_outputs(modified_nn_light_reg, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IvVtz81Lqt5",
        "outputId": "71bdba1e-54ef-4483-b929-6a1e9da370f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher accuracy: 92.92%\n",
            "Student accuracy without teacher: 91.25%\n",
            "Student accuracy with CE + KD: 91.46%\n",
            "Student accuracy with CE + RegressorMSE: 91.28%\n"
          ]
        }
      ],
      "source": [
        "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
        "print(f\"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%\")\n",
        "print(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%\")\n",
        "\n",
        "print(f\"Student accuracy with CE + RegressorMSE: {test_accuracy_light_ce_and_mse_loss:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tmiAP8lLw2p"
      },
      "source": [
        "relation based distillation using the concept of fsp and mse loss\n",
        "\n",
        "The FSP is calculated between the feature vectors of the fully connected layers of both teacher and the student models and then calculating the loss between those two matrices using mse.\n",
        "\n",
        "I am also using a regressor to make the dimensions of the fully connected layers of teacher similar to student ones since the architecture of both the models are different."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WO2WnLc-L4Ij"
      },
      "outputs": [],
      "source": [
        "class ModifiedDeepNNCosine(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ModifiedDeepNNCosine, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 512)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        feat1 = self.fc1(x)\n",
        "        out = self.fc2(self.dropout(self.relu(feat1)))\n",
        "        return out, feat1\n",
        "\n",
        "\n",
        "class ModifiedLightNNCosine(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ModifiedLightNNCosine, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        self.fc1 = nn.Linear(16 * 7 * 7, 256)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        feat1 = self.fc1(x)\n",
        "        out = self.fc2(self.dropout(self.relu(feat1)))\n",
        "        return out, feat1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59hDhh5Kf0qw"
      },
      "outputs": [],
      "source": [
        "class TeacherRegressor(nn.Module):\n",
        "    def __init__(self, in_dim=512, out_dim=256):\n",
        "        super(TeacherRegressor, self).__init__()\n",
        "        self.proj = nn.Linear(in_dim, out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.proj(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWMow_lbgQKL"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDJrjklNckEc"
      },
      "outputs": [],
      "source": [
        "def compute_fsp_matrix(A, B):\n",
        "    \"\"\"\n",
        "    A: (batch_size, d1)\n",
        "    B: (batch_size, d2)\n",
        "    Returns: (d1, d2) FSP matrix\n",
        "    \"\"\"\n",
        "    return torch.matmul(A.T, B) / A.size(0)\n",
        "\n",
        "def train_relation_distillation(teacher, student, regressor, train_loader, epochs, learning_rate, alpha, ce_weight, device):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(list(student.parameters()) + list(regressor.parameters()), lr=learning_rate)\n",
        "\n",
        "    teacher.eval()\n",
        "    student.train()\n",
        "    regressor.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                teacher_logits, teacher_feat = teacher(images)\n",
        "\n",
        "            student_logits, student_feat = student(images)\n",
        "\n",
        "            # Project teacher_feat to match student_feat dimensionality\n",
        "            teacher_feat_projected = regressor(teacher_feat)\n",
        "\n",
        "            # Compute FSP\n",
        "            fsp_teacher = compute_fsp_matrix(teacher_feat_projected, teacher_feat_projected)\n",
        "            fsp_student = compute_fsp_matrix(student_feat, student_feat)\n",
        "\n",
        "            mse_loss = nn.MSELoss()(fsp_student, fsp_teacher)\n",
        "            ce_loss = criterion(student_logits, labels)\n",
        "\n",
        "            loss = alpha * mse_loss + ce_weight * ce_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHn-2157ckA7"
      },
      "outputs": [],
      "source": [
        "def test(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs, _ = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXOetOIucj9s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a14642c1-f6c1-4738-ce0f-89829bcdfe39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.7992\n",
            "Epoch [2/10], Loss: 0.1290\n",
            "Epoch [3/10], Loss: 0.1062\n",
            "Epoch [4/10], Loss: 0.0925\n",
            "Epoch [5/10], Loss: 0.0823\n",
            "Epoch [6/10], Loss: 0.0745\n",
            "Epoch [7/10], Loss: 0.0677\n",
            "Epoch [8/10], Loss: 0.0621\n",
            "Epoch [9/10], Loss: 0.0564\n",
            "Epoch [10/10], Loss: 0.0532\n",
            "Test Accuracy: 91.66%\n",
            "Student Accuracy with FSP-based KD: 91.66%\n"
          ]
        }
      ],
      "source": [
        "# # Assuming nn_deep is already trained and available\n",
        "# modified_nn_deep = ModifiedDeepNNCosine(num_classes=10).to(device)\n",
        "# modified_nn_deep.load_state_dict(nn_deep.state_dict())\n",
        "\n",
        "# # Fresh student model\n",
        "# torch.manual_seed(42)\n",
        "# modified_nn_light = ModifiedLightNNCosine(num_classes=10).to(device)\n",
        "\n",
        "# # Initialize a ModifiedLightNNRegressor\n",
        "# torch.manual_seed(42)\n",
        "# modified_nn_light_reg = ModifiedLightNNRegressor(num_classes=10).to(device)\n",
        "\n",
        "# # We do not have to train the modified deep network from scratch of course, we just load its weights from the trained instance\n",
        "# modified_nn_deep_reg = ModifiedDeepNNRegressor(num_classes=10).to(device)\n",
        "# modified_nn_deep_reg.load_state_dict(nn_deep.state_dict())\n",
        "\n",
        "# Step 1: Initialize ModifiedDeepNNCosine\n",
        "modified_nn_deep = ModifiedDeepNNCosine(num_classes=10).to(device)\n",
        "\n",
        "# Step 2: Copy compatible layers manually\n",
        "with torch.no_grad():\n",
        "    # Copy convolutional layers (same structure)\n",
        "    modified_nn_deep.features.load_state_dict(nn_deep.features.state_dict())\n",
        "\n",
        "    # Copy classifier layers manually\n",
        "    modified_nn_deep.fc1.weight.copy_(nn_deep.classifier[0].weight)\n",
        "    modified_nn_deep.fc1.bias.copy_(nn_deep.classifier[0].bias)\n",
        "\n",
        "    modified_nn_deep.fc2.weight.copy_(nn_deep.classifier[3].weight)\n",
        "    modified_nn_deep.fc2.bias.copy_(nn_deep.classifier[3].bias)\n",
        "\n",
        "\n",
        "# Step 1: Initialize the modified student model\n",
        "modified_nn_light = ModifiedLightNNCosine(num_classes=10).to(device)\n",
        "\n",
        "# Step 2: Copy weights from LightNN to ModifiedLightNNCosine\n",
        "with torch.no_grad():\n",
        "    # Copy convolutional layers (features)\n",
        "    modified_nn_light.features.load_state_dict(nn_light.features.state_dict())\n",
        "\n",
        "    # Copy classifier layers manually\n",
        "    modified_nn_light.fc1.weight.copy_(nn_light.classifier[0].weight)\n",
        "    modified_nn_light.fc1.bias.copy_(nn_light.classifier[0].bias)\n",
        "\n",
        "    modified_nn_light.fc2.weight.copy_(nn_light.classifier[3].weight)\n",
        "    modified_nn_light.fc2.bias.copy_(nn_light.classifier[3].bias)\n",
        "\n",
        "\n",
        "\n",
        "# ðŸ”¥ Initialize the regressor to project teacher features to student feature size\n",
        "regressor = TeacherRegressor(in_dim=512, out_dim=256).to(device)\n",
        "\n",
        "# Train student with FSP-based relation distillation\n",
        "train_relation_distillation(\n",
        "    teacher=modified_nn_deep,\n",
        "    student=modified_nn_light,\n",
        "    regressor=regressor,\n",
        "    train_loader=train_loader,\n",
        "    epochs=10,\n",
        "    learning_rate=0.001,\n",
        "    alpha=0.5,      # weight for FSP loss\n",
        "    ce_weight=0.5,  # weight for cross-entropy\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Test student\n",
        "test_accuracy_relation_kd = test(modified_nn_light, test_loader, device)\n",
        "print(f\"Student Accuracy with FSP-based KD: {test_accuracy_relation_kd:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n7EvcE-F-BiY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}