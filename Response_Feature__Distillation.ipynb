{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-wRkp31ioMF",
        "outputId": "a6cd3176-b31f-4c03-f4de-fa56b0e6c876"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YFjC6sOjgHH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Response Based Distillation on Fashion MNIST dataset\n",
        "It uses the softened probabilities difference between the teacher and the student models to compute the distillation loss"
      ],
      "metadata": {
        "id": "XP-6gz7Pzs0_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXQoXGlRnMmt",
        "outputId": "3be3d01a-3894-47b7-a115-0ae7319cf4dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 17.9MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 268kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 4.94MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 9.55MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 0.44664344512450416\n",
            "Epoch 2/10, Loss: 0.25785705035746986\n",
            "Epoch 3/10, Loss: 0.21270009479733673\n",
            "Epoch 4/10, Loss: 0.180030266120871\n",
            "Epoch 5/10, Loss: 0.1544160482519344\n",
            "Epoch 6/10, Loss: 0.13343401438296476\n",
            "Epoch 7/10, Loss: 0.11114704581910867\n",
            "Epoch 8/10, Loss: 0.09539505504945447\n",
            "Epoch 9/10, Loss: 0.07704926771757159\n",
            "Epoch 10/10, Loss: 0.06815543323000675\n",
            "Test Accuracy: 92.61%\n",
            "Norm of 1st layer of nn_light: 2.3761110305786133\n",
            "Norm of 1st layer of new_nn_light: 2.3761110305786133\n",
            "DeepNN parameters: 938,922\n",
            "LightNN parameters: 206,010\n",
            "Epoch 1/10, Loss: 0.5245428934915742\n",
            "Epoch 2/10, Loss: 0.337614206045167\n",
            "Epoch 3/10, Loss: 0.2914585894358946\n",
            "Epoch 4/10, Loss: 0.2637054169578339\n",
            "Epoch 5/10, Loss: 0.24345697635717228\n",
            "Epoch 6/10, Loss: 0.2239762470602735\n",
            "Epoch 7/10, Loss: 0.20901942284884992\n",
            "Epoch 8/10, Loss: 0.1962912838612157\n",
            "Epoch 9/10, Loss: 0.18218423775645462\n",
            "Epoch 10/10, Loss: 0.17100741217004212\n",
            "Test Accuracy: 91.13%\n",
            "Teacher accuracy: 92.61%\n",
            "Student accuracy: 91.13%\n",
            "Epoch 1/10, Loss: 0.8522997229719467\n",
            "Epoch 2/10, Loss: 0.507976720200927\n",
            "Epoch 3/10, Loss: 0.42678613643020963\n",
            "Epoch 4/10, Loss: 0.37419506799437596\n",
            "Epoch 5/10, Loss: 0.3392098479624242\n",
            "Epoch 6/10, Loss: 0.309909281287112\n",
            "Epoch 7/10, Loss: 0.2862839816348639\n",
            "Epoch 8/10, Loss: 0.26586626574937217\n",
            "Epoch 9/10, Loss: 0.2471832897363187\n",
            "Epoch 10/10, Loss: 0.2310501549289679\n",
            "Test Accuracy: 90.91%\n",
            "Teacher accuracy: 92.61%\n",
            "Student accuracy without teacher: 91.13%\n",
            "Student accuracy with CE + KD: 90.91%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "# Check device (supports torch.accelerator if available, else CPU)\n",
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Preprocessing for FashionMNIST (grayscale, so mean and std are single values)\n",
        "transforms_mnist = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.2860], std=[0.3530]),  # typical mean/std for FashionMNIST\n",
        "])\n",
        "\n",
        "# Loading FashionMNIST dataset:\n",
        "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms_mnist)\n",
        "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transforms_mnist)\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "# Adjusted DeepNN and LightNN for single channel input (1 channel instead of 3)\n",
        "\n",
        "class DeepNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(DeepNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 128, kernel_size=3, padding=1),  # input channel 1\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(32 * 7 * 7, 512),  # 28x28 images downsampled twice by factor 2 -> 7x7\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "class LightNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(LightNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1),  # input channel 1\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(16 * 7 * 7, 256),  # similarly 7x7 size after pooling\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Training and testing functions remain unchanged\n",
        "\n",
        "def train(model, train_loader, epochs, learning_rate, device):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "def test(model, test_loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Initialize and train teacher (DeepNN)\n",
        "nn_deep = DeepNN(num_classes=10).to(device)\n",
        "train(nn_deep, train_loader, epochs=10, learning_rate=0.001, device=device)\n",
        "test_accuracy_deep = test(nn_deep, test_loader, device)\n",
        "\n",
        "# Initialize student (LightNN)\n",
        "torch.manual_seed(42)\n",
        "nn_light = LightNN(num_classes=10).to(device)\n",
        "\n",
        "# Print norm of first layer weights for sanity check\n",
        "torch.manual_seed(42)\n",
        "new_nn_light = LightNN(num_classes=10).to(device)\n",
        "print(\"Norm of 1st layer of nn_light:\", torch.norm(nn_light.features[0].weight).item())\n",
        "print(\"Norm of 1st layer of new_nn_light:\", torch.norm(new_nn_light.features[0].weight).item())\n",
        "\n",
        "# Print total parameters\n",
        "total_params_deep = \"{:,}\".format(sum(p.numel() for p in nn_deep.parameters()))\n",
        "print(f\"DeepNN parameters: {total_params_deep}\")\n",
        "total_params_light = \"{:,}\".format(sum(p.numel() for p in nn_light.parameters()))\n",
        "print(f\"LightNN parameters: {total_params_light}\")\n",
        "\n",
        "# Train student\n",
        "train(nn_light, train_loader, epochs=10, learning_rate=0.001, device=device)\n",
        "test_accuracy_light_ce = test(nn_light, test_loader, device)\n",
        "\n",
        "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
        "print(f\"Student accuracy: {test_accuracy_light_ce:.2f}%\")\n",
        "\n",
        "# Knowledge Distillation training function remains unchanged\n",
        "\n",
        "def train_knowledge_distillation(teacher, student, train_loader, epochs, learning_rate, T, soft_target_loss_weight, ce_loss_weight, device):\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
        "\n",
        "    teacher.eval()\n",
        "    student.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                teacher_logits = teacher(inputs)\n",
        "\n",
        "            student_logits = student(inputs)\n",
        "\n",
        "            soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n",
        "            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
        "\n",
        "            soft_targets_loss = torch.sum(soft_targets * (soft_targets.log() - soft_prob)) / soft_prob.size()[0] * (T**2)\n",
        "            label_loss = ce_loss(student_logits, labels)\n",
        "\n",
        "            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "# Run knowledge distillation training\n",
        "\n",
        "train_knowledge_distillation(\n",
        "    teacher=nn_deep,\n",
        "    student=new_nn_light,\n",
        "    train_loader=train_loader,\n",
        "    epochs=10,\n",
        "    learning_rate=0.001,\n",
        "    T=2,\n",
        "    soft_target_loss_weight=0.25,\n",
        "    ce_loss_weight=0.75,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "test_accuracy_light_ce_and_kd = test(new_nn_light, test_loader, device)\n",
        "\n",
        "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
        "print(f\"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%\")\n",
        "print(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYbNGp29Htje"
      },
      "source": [
        "This is the code for the Feature based distillation loss using the extracted features of the intermediate layer after the convolutions and before fully connected layers in FashionMNIST dataset using cosine loss\n",
        "\n",
        "Noting to first making the shapes of features of both models equal to compute the loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVF4hvrXchuj"
      },
      "outputs": [],
      "source": [
        "class ModifiedDeepNNCosine(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ModifiedDeepNNCosine, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 128, kernel_size=3, padding=1),  # Changed to 1 channel\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(32 * 7 * 7, 512),  # 28→14→7 after two 2×2 maxpools\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        flattened_conv_output = torch.flatten(x, 1)\n",
        "        x = self.classifier(flattened_conv_output)\n",
        "        flattened_conv_output_after_pooling = torch.nn.functional.avg_pool1d(flattened_conv_output, 2)\n",
        "        return x, flattened_conv_output_after_pooling\n",
        "class ModifiedLightNNCosine(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ModifiedLightNNCosine, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1),  # Changed to 1 channel\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(16 * 7 * 7, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        flattened_conv_output = torch.flatten(x, 1)\n",
        "        x = self.classifier(flattened_conv_output)\n",
        "        return x, flattened_conv_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTNAuEpucnEH",
        "outputId": "903377e0-2f3a-4ba3-9d0b-43c596f79f4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load weights from original trained teacher (DeepNN → ModifiedDeepNNCosine)\n",
        "modified_nn_deep = ModifiedDeepNNCosine(num_classes=10).to(device)\n",
        "modified_nn_deep.load_state_dict(nn_deep.state_dict())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwOUuVeldKuz"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "modified_nn_light = ModifiedLightNNCosine(num_classes=10).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PDMM-ETc-im",
        "outputId": "3dba58d9-9526-4de2-8eac-dd839a520e60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Norm of 1st layer for original DeepNN: 7.127074718475342\n",
            "Norm of 1st layer for ModifiedDeepNNCosine: 7.127074718475342\n",
            "Norm of 1st layer for ModifiedLightNNCosine: 2.3761110305786133\n"
          ]
        }
      ],
      "source": [
        "print(\"Norm of 1st layer for original DeepNN:\", torch.norm(nn_deep.features[0].weight).item())\n",
        "print(\"Norm of 1st layer for ModifiedDeepNNCosine:\", torch.norm(modified_nn_deep.features[0].weight).item())\n",
        "print(\"Norm of 1st layer for ModifiedLightNNCosine:\", torch.norm(modified_nn_light.features[0].weight).item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOHBzFK0dZGw",
        "outputId": "8929c87e-ddfc-4819-d49b-3e3a9a48a3b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Student logits shape: torch.Size([128, 10])\n",
            "Student hidden representation shape: torch.Size([128, 784])\n",
            "Teacher logits shape: torch.Size([128, 10])\n",
            "Teacher hidden representation shape: torch.Size([128, 784])\n"
          ]
        }
      ],
      "source": [
        "# Create a sample input tensor\n",
        "sample_input = torch.randn(128, 1, 28, 28).to(device)\n",
        "\n",
        "# Pass the input through the student\n",
        "logits, hidden_representation = modified_nn_light(sample_input)\n",
        "\n",
        "# Print the shapes of the tensors\n",
        "print(\"Student logits shape:\", logits.shape) # batch_size x total_classes\n",
        "print(\"Student hidden representation shape:\", hidden_representation.shape) # batch_size x hidden_representation_size\n",
        "\n",
        "# Pass the input through the teacher\n",
        "logits, hidden_representation = modified_nn_deep(sample_input)\n",
        "\n",
        "# Print the shapes of the tensors\n",
        "print(\"Teacher logits shape:\", logits.shape) # batch_size x total_classes\n",
        "print(\"Teacher hidden representation shape:\", hidden_representation.shape) # batch_size x hidden_representation_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "my1OAZFSe50a"
      },
      "outputs": [],
      "source": [
        "def train_cosine_loss(teacher, student, train_loader, epochs, learning_rate, hidden_rep_loss_weight, ce_loss_weight, device):\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "    cosine_loss = nn.CosineEmbeddingLoss()\n",
        "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
        "\n",
        "    teacher.to(device)\n",
        "    student.to(device)\n",
        "    teacher.eval()  # Teacher set to evaluation mode\n",
        "    student.train() # Student to train mode\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with the teacher model and keep only the hidden representation\n",
        "            with torch.no_grad():\n",
        "                _, teacher_hidden_representation = teacher(inputs)\n",
        "\n",
        "            # Forward pass with the student model\n",
        "            student_logits, student_hidden_representation = student(inputs)\n",
        "\n",
        "            # Calculate the cosine loss. Target is a vector of ones. From the loss formula above we can see that is the case where loss minimization leads to cosine similarity increase.\n",
        "            hidden_rep_loss = cosine_loss(student_hidden_representation, teacher_hidden_representation, target=torch.ones(inputs.size(0)).to(device))\n",
        "\n",
        "            # Calculate the true label loss\n",
        "            label_loss = ce_loss(student_logits, labels)\n",
        "\n",
        "            # Weighted sum of the two losses\n",
        "            loss = hidden_rep_loss_weight * hidden_rep_loss + ce_loss_weight * label_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPLarQ_he7la"
      },
      "outputs": [],
      "source": [
        "def test_multiple_outputs(model, test_loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs, _ = model(inputs) # Disregard the second tensor of the tuple\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Knt02tK0eV8L",
        "outputId": "c0d8230b-9aa0-488e-c7cd-7ad082756d04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.5623220297446383\n",
            "Epoch 2/10, Loss: 0.4239960838990933\n",
            "Epoch 3/10, Loss: 0.38612460397453957\n",
            "Epoch 4/10, Loss: 0.362794399325019\n",
            "Epoch 5/10, Loss: 0.3483948197319055\n",
            "Epoch 6/10, Loss: 0.33601962214212683\n",
            "Epoch 7/10, Loss: 0.32584613224844944\n",
            "Epoch 8/10, Loss: 0.31690224973377645\n",
            "Epoch 9/10, Loss: 0.30871780801302334\n",
            "Epoch 10/10, Loss: 0.3020535359250457\n",
            "Test Accuracy: 91.19%\n",
            "Student accuracy with CE + Cosine Loss: 91.19%\n"
          ]
        }
      ],
      "source": [
        "train_cosine_loss(\n",
        "    teacher=modified_nn_deep,\n",
        "    student=modified_nn_light,\n",
        "    train_loader=train_loader,\n",
        "    epochs=10,\n",
        "    learning_rate=0.001,\n",
        "    hidden_rep_loss_weight=0.25,\n",
        "    ce_loss_weight=0.75,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "test_accuracy_light_ce_and_cosine_loss = test_multiple_outputs(modified_nn_light, test_loader, device)\n",
        "print(f\"Student accuracy with CE + Cosine Loss: {test_accuracy_light_ce_and_cosine_loss:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BaeeXyxkK085"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}