{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65ff125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "499fff7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8., 15.])\n"
     ]
    }
   ],
   "source": [
    "x=torch.Tensor([2,3])\n",
    "y=torch.Tensor([4,5])\n",
    "print(x*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "140d4abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.zeros([3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2c2e223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9801f65e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9544d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=torch.rand([4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b8a82fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6590, 0.5981, 0.5561, 0.5707, 0.1230],\n",
      "        [0.8470, 0.8749, 0.7548, 0.3238, 0.5399],\n",
      "        [0.0507, 0.9516, 0.0466, 0.8328, 0.8832],\n",
      "        [0.5707, 0.5996, 0.1355, 0.0504, 0.2328]])\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9650abc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6590, 0.5981, 0.5561, 0.5707, 0.1230, 0.8470, 0.8749, 0.7548, 0.3238,\n",
      "         0.5399, 0.0507, 0.9516, 0.0466, 0.8328, 0.8832, 0.5707, 0.5996, 0.1355,\n",
      "         0.0504, 0.2328]])\n"
     ]
    }
   ],
   "source": [
    "y=y.view([1,20])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23341c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6590, 0.5981, 0.5561, 0.5707],\n",
      "        [0.1230, 0.8470, 0.8749, 0.7548],\n",
      "        [0.3238, 0.5399, 0.0507, 0.9516],\n",
      "        [0.0466, 0.8328, 0.8832, 0.5707],\n",
      "        [0.5996, 0.1355, 0.0504, 0.2328]])\n"
     ]
    }
   ],
   "source": [
    "y=y.view([5,4])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26f429fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5486fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fbec9b",
   "metadata": {},
   "source": [
    "DATA- AQUISITION OF DATA, PREPROCESSING THE DATA AND ITERATE OVER DATA\n",
    "TORCHVISION - COLLECTION OF DATA THAT IS USED FOR VISION TASK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b002f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c33e7fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=datasets.MNIST('',train=True, download=True, transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2349da2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=datasets.MNIST('',train=False,download=True,transform=transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c811753",
   "metadata": {},
   "source": [
    "load this data into another type of object which can help us to iterate over this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5290a855",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset=torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset=torch.utils.data.DataLoader(test, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007b7a2d",
   "metadata": {},
   "source": [
    "batch_size - how many items you are feeding into the model at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4017cdf7",
   "metadata": {},
   "source": [
    "shuffle - to reduce overfitting and generalisation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516a29f8",
   "metadata": {},
   "source": [
    "*iterating over data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8ddf97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([1, 4, 3, 0, 5, 8, 1, 8, 5, 2])]\n"
     ]
    }
   ],
   "source": [
    "for data in trainset:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "455fd7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4)\n"
     ]
    }
   ],
   "source": [
    "x,y = data[0][1],data[1][1]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efa35995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4da2911a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x.view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9146d4",
   "metadata": {},
   "source": [
    "balancing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7429a341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n"
     ]
    }
   ],
   "source": [
    "total=0\n",
    "counter_dict={0:0,1:0,2:0,3:0,4:0,5:0,6:0,7:0,8:0,9:0}\n",
    "for data in trainset:\n",
    "    xs,ys = data\n",
    "    for y in ys:\n",
    "        counter_dict[int(y)]+=1\n",
    "        total+=1\n",
    "\n",
    "print(counter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "436df1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:9.871666666666666\n",
      "1:11.236666666666666\n",
      "2:9.93\n",
      "3:10.218333333333334\n",
      "4:9.736666666666666\n",
      "5:9.035\n",
      "6:9.863333333333333\n",
      "7:10.441666666666666\n",
      "8:9.751666666666667\n",
      "9:9.915000000000001\n"
     ]
    }
   ],
   "source": [
    "for i in counter_dict:\n",
    "    print(f'{i}:{counter_dict[i]/total*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "514ca376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn  #like OOPS\n",
    "import torch.nn.functional as F  #like functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f489bd2f",
   "metadata": {},
   "source": [
    "in functional you will always has to pass parameters and for nns you will just initialize things"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac810c4",
   "metadata": {},
   "source": [
    "**model building**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3e8ae4",
   "metadata": {},
   "source": [
    "The torch.nn import gives us access to some helpful neural network things, such as various neural network layer types (things like regular fully-connected layers, convolutional layers (for imagery), recurrent layers...etc). For now, we've only spoken about fully-connected layers, so we will just be using those for now.\n",
    "\n",
    "The torch.nn.functional area specifically gives us access to some handy functions that we might not want to write ourselves. We will be using the relu or \"rectified linear\" activation function for our neurons. Instead of writing all of the code for these things, we can just import them, since these are things everyone will be needing in their deep learning code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ecf992c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):  #inherit from nn. module\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()  #running the intialization for nn.module and whatevefr you had put in __init__ runs the initialization method of the parent class from where are you inheriting\n",
    "        # self.fc1=nn.Linear(input,output)\n",
    "        # three layers of 64 neurons\n",
    "        self.fc1=nn.Linear(28*28,64)  #for convolutional nn.calm\n",
    "        self.fc2=nn.Linear(64,64)\n",
    "        self.fc3=nn.Linear(64,64)\n",
    "        self.fc4=nn.Linear(64,10)\n",
    "\n",
    "    # building a feed-forward neural network means that the data is passing in one direction\n",
    "    def forward(self,x):   #x=input dataset\n",
    "        # x=self.fc1(x)  #we forgot our activation function so the data will not be scaled properly\n",
    "        x=F.relu(self.fc1(x))   #relu-rectified linear- activation function\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=F.relu(self.fc3(x))\n",
    "        x=self.fc4(x)\n",
    "\n",
    "        return F.log_softmax(x,dim=1)\n",
    "\n",
    "\n",
    "net=Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c352a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=torch.rand(28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "033ca005",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.view(-1,28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0616eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d582a575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2231, -2.3630, -2.3366, -2.2171, -2.4605, -2.4358, -2.2301, -2.1275,\n",
       "         -2.3120, -2.3711]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b29842",
   "metadata": {},
   "source": [
    "loss-measure of how wrong is the model goal loss to decrease\n",
    "optimizer- adjust weights and biases based on the loss and the gradients\n",
    "time to reduce the loss with the help of optimizers - learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "666034fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0040, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1722, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0165, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer=optim.Adam(net.parameters(), lr= 0.001)\n",
    "# net.parameters[]- everything that can be adjustible in the model\n",
    "# learning rate-steps you are taking to reach the minima of the optimization curve\n",
    "# we do not optimize for accuracy we optimize for loss\n",
    "# a full pass through a data is called epoch\n",
    "EPOCHS=3\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in trainset:\n",
    "        X,y = data\n",
    "        net.zero_grad()\n",
    "# why to pass data in batches- it reduces the training time and helps in generalizing\n",
    "        output = net(X.view(-1, 28*28))\n",
    "        loss= F.nll_loss(output, y)\n",
    "        # there are two ways to calculate loss one is  based on one-hot vectors but our dataset is a scalar value with just one hot use nll_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()  #this willa djust the weights for us\n",
    "\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fbb23885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  98.79\n"
     ]
    }
   ],
   "source": [
    "correct=0\n",
    "total = 0\n",
    "# validate our data testing data do not optimize based on this data and see the accuracy of the model\n",
    "with torch.no_grad():\n",
    "    for data in trainset:\n",
    "        X,y = data\n",
    "        output=net(X.view(-1,28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i)==y[idx]:\n",
    "                correct+=1\n",
    "            total+=1\n",
    "\n",
    "print(\"Accuracy: \",round(correct/total*100,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29979bbc",
   "metadata": {},
   "source": [
    "***CONVOLUTIONAL LAYERS***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19331c26",
   "metadata": {},
   "source": [
    "convolutional neural networks are doing better than the recurrent neural networks int he sequential types of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51422e5",
   "metadata": {},
   "source": [
    "for the convolutional layers you will have to pass the image as it is and not like to flatten it as we have done for fully connected layers\n",
    "\n",
    "when we convolve an image we are trying to extract features of that image. A window = kernel. extract featrues and then generates a scalar\n",
    "\n",
    "After convolution do pooling= max pooling(general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5411bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6807ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cf765b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c7008c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81a4aac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1012/1012 [00:11<00:00, 84.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1013/1013 [00:09<00:00, 111.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CATS:  1011\n",
      "DOGS:  1012\n"
     ]
    }
   ],
   "source": [
    "REBUILD_DATA=True\n",
    "\n",
    "class DogsVSCats():\n",
    "    IMG_SIZE=50\n",
    "    CATS = 'cats'\n",
    "    DOGS = 'dogs'\n",
    "    LABELS = {CATS:0, DOGS:1}\n",
    "    catcount=0\n",
    "    dogcount=0\n",
    "    training_data=[]\n",
    "    def make_training_data(self):  #iterating over the directory\n",
    "        for label in self.LABELS:\n",
    "            print(label)\n",
    "            for f in tqdm(os.listdir(label)):   \n",
    "                #iterating over the images in the directories\n",
    "                try:\n",
    "                    # (os.listdir(label) it is the directory\n",
    "                    path=os.path.join(label, f)\n",
    "                    img= cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "                    # color add channels to the conv net but not the dimensions\n",
    "                    img=cv2.resize(img,(self.IMG_SIZE,self.IMG_SIZE))\n",
    "                    self.training_data.append([np.array(img),np.eye(2)[self.LABELS[label]]])\n",
    "                    if label == self.CATS:\n",
    "                        self.catcount +=1\n",
    "                    elif label == self.DOGS:\n",
    "                        self.dogcount +=1\n",
    "            \n",
    "                except Exception as e:\n",
    "                    pass\n",
    "\n",
    "        np.random.shuffle(self.training_data)\n",
    "        np.save('training_data.npy', np.array(self.training_data, dtype=object), allow_pickle=True)\n",
    "        print('CATS: ', self.catcount)\n",
    "        print('DOGS: ', self.dogcount)\n",
    "            \n",
    "if REBUILD_DATA:\n",
    "    dogsvscats = DogsVSCats()\n",
    "    dogsvscats.make_training_data()\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d84fffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.load(\"training_data.npy\",allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "38e546ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8ab3999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[114, 110,  93, ...,  65,  66,  72],\n",
      "        [116, 111,  95, ...,  72,  70,  73],\n",
      "        [116, 109, 103, ...,  70,  70,  74],\n",
      "        ...,\n",
      "        [138, 135, 127, ..., 170, 255, 255],\n",
      "        [142, 135, 115, ..., 159, 253, 255],\n",
      "        [141, 133, 136, ..., 159, 254, 254]], dtype=uint8) array([1., 0.])]\n"
     ]
    }
   ],
   "source": [
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76b4c7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0.]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(training_data[0][0], cmap='gray')\n",
    "plt.show()\n",
    "print(training_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ee8de65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "715030f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module) :\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,32,5)   #5= kernel size\n",
    "        self.conv2 = nn.Conv2d(32,64,5)\n",
    "        self.conv3 = nn.Conv2d(64,128,5)\n",
    "        \n",
    "        x=torch.randn(50,50).view(-1,1,50,50)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "        self.fc1 = nn.Linear(self._to_linear,512)\n",
    "        self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "    def convs(self, x):\n",
    "        x=F.max_pool2d(F.relu(self.conv1(x)),(2,2))\n",
    "        x=F.max_pool2d(F.relu(self.conv2(x)),(2,2))\n",
    "        x=F.max_pool2d(F.relu(self.conv3(x)),(2,2))\n",
    "\n",
    "        print(x[0].shape)\n",
    "\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear= x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)\n",
    "         # so we reshape to be flattened as when it comes out of the convs  it is not flat yet\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "net=Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2834688b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\archa\\AppData\\Local\\Temp\\ipykernel_19020\\2499149120.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  x=torch.Tensor([i[0] for i in training_data]).view(-1,50,50)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.001)\n",
    "loss_function=nn.MSELoss()\n",
    "\n",
    "x=torch.Tensor([i[0] for i in training_data]).view(-1,50,50)\n",
    "x=x/255\n",
    "y=torch.Tensor([i[1] for i in training_data])\n",
    "\n",
    "VAL_PCT = 0.1\n",
    "val_size = int(len(x)*VAL_PCT)\n",
    "print(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc11377d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1821\n",
      "202\n"
     ]
    }
   ],
   "source": [
    "train_x = x[:-val_size]\n",
    "train_y = y[:-val_size]\n",
    "test_x = x[-val_size:]\n",
    "test_y = y[-val_size:]\n",
    "print(len(train_x))\n",
    "print(len(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3765cacc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1821, 2])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9556f961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/19 [00:00<00:05,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 2/19 [00:00<00:04,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 4/19 [00:00<00:03,  4.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 6/19 [00:01<00:02,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 8/19 [00:01<00:02,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 10/19 [00:02<00:01,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 12/19 [00:02<00:01,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 14/19 [00:02<00:00,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 16/19 [00:03<00:00,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:03<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/19 [00:00<00:03,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 3/19 [00:00<00:02,  6.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 5/19 [00:00<00:02,  6.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 7/19 [00:01<00:01,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 9/19 [00:01<00:01,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 11/19 [00:01<00:01,  5.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 13/19 [00:02<00:01,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 15/19 [00:02<00:00,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 17/19 [00:02<00:00,  5.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:03<00:00,  6.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 2])\n",
      "tensor(0.2523, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Batch_size = 100\n",
    "EPOCHS = 2\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in tqdm(range(0,len(train_x),Batch_size)):\n",
    "        # print(i,i+Batch_size)\n",
    "        # we are going to start from 0 till len(train_x and we will take the stpes of BATCH_SIZE )\n",
    "        batch_x = train_x[i:i+Batch_size].view(-1,1,50,50)\n",
    "        batch_y = train_y[i:i+Batch_size]\n",
    "        # fitment optimization - zero the gradients\n",
    "        \n",
    "        net.zero_grad()\n",
    "        outputs = net(batch_x)\n",
    "        loss=loss_function(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5401bd21",
   "metadata": {},
   "source": [
    "need to learn about .eval() and .train() and about dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "345a8260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 56/202 [00:00<00:00, 305.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 202/202 [00:00<00:00, 456.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "torch.Size([128, 2, 2])\n",
      "ACCURACY =  0.505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(test_x))):\n",
    "        real_class = torch.argmax(test_y[i])\n",
    "        net_out = net(test_x[i].view(-1,1,50,50))[0]\n",
    "        predicted_class = torch.argmax(net_out)\n",
    "        if predicted_class == real_class:\n",
    "            correct+=1\n",
    "        total+=1\n",
    "\n",
    "print('ACCURACY = ', round(correct/total,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5bfae4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a1447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9fcce46f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1248460579.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[34], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    .to(device)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    ".to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3dc48041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('running on cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e40aac29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7863208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net):\n",
    "    optimizer = optim.Adam(net.parameters(),lr=0.001)\n",
    "    loss_function=nn.MSELoss()\n",
    "    for epoch in range(EPOCHS):\n",
    "    for i in tqdm(range(0,len(train_x),Batch_size)):\n",
    "        # print(i,i+Batch_size)\n",
    "        # we are going to start from 0 till len(train_x and we will take the stpes of BATCH_SIZE )\n",
    "        batch_x = train_x[i:i+Batch_size].view(-1,1,50,50)\n",
    "        batch_y = train_y[i:i+Batch_size]\n",
    "        # fitment optimization - zero the gradients\n",
    "        \n",
    "        net.zero_grad()\n",
    "        outputs = net(batch_x)\n",
    "        loss=loss_function(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(loss)\n",
    "\n",
    "train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342125bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net):\n",
    "    # testing\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(test_x))):\n",
    "            real_class = torch.argmax(test_y[i]).to(device)\n",
    "            net_out = net(test_x[i].view(-1,1,50,50).to(device))[0]\n",
    "            predicted_class = torch.argmax(net_out)\n",
    "            if predicted_class == real_class:\n",
    "                correct+=1\n",
    "            total+=1\n",
    "\n",
    "    print('ACCURACY = ', round(correct/total,3))\n",
    "\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa83ff1",
   "metadata": {},
   "source": [
    "IN AND OUT SAMPLE ACCURACY AND LOSS - THESE FOUR THINGS TELLS THAT HOW LONG WE SHOULD TRAIN THE MODEL AND WHAT MODEL IS THE BEST IF WE ARE TRYING TO SOLVE THE SAME TASK WITH DIFFERENT MODELS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c1674b",
   "metadata": {},
   "source": [
    "we want to do the test while we are training / granular testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da0a283f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023\n",
      "torch.Size([128, 2, 2])\n",
      "202\n",
      "1821\n",
      "202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\archa\\AppData\\Local\\Temp\\ipykernel_20224\\3342240839.py:91: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  x=torch.Tensor([i[0] for i in training_data]).view(-1,50,50)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "REBUILD_DATA=False\n",
    "\n",
    "class DogsVSCats():\n",
    "    IMG_SIZE=50\n",
    "    CATS = 'cats'\n",
    "    DOGS = 'dogs'\n",
    "    LABELS = {CATS:0, DOGS:1}\n",
    "    catcount=0\n",
    "    dogcount=0\n",
    "    training_data=[]\n",
    "    def make_training_data(self):  #iterating over the directory\n",
    "        for label in self.LABELS:\n",
    "            print(label)\n",
    "            for f in tqdm(os.listdir(label)):   \n",
    "                #iterating over the images in the directories\n",
    "                try:\n",
    "                    # (os.listdir(label) it is the directory\n",
    "                    path=os.path.join(label, f)\n",
    "                    img= cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "                    # color add channels to the conv net but not the dimensions\n",
    "                    img=cv2.resize(img,(self.IMG_SIZE,self.IMG_SIZE))\n",
    "                    self.training_data.append([np.array(img),np.eye(2)[self.LABELS[label]]])\n",
    "                    if label == self.CATS:\n",
    "                        self.catcount +=1\n",
    "                    elif label == self.DOGS:\n",
    "                        self.dogcount +=1\n",
    "            \n",
    "                except Exception as e:\n",
    "                    pass\n",
    "\n",
    "        np.random.shuffle(self.training_data)\n",
    "        np.save('training_data.npy', np.array(self.training_data, dtype=object), allow_pickle=True)\n",
    "        print('CATS: ', self.catcount)\n",
    "        print('DOGS: ', self.dogcount)\n",
    "            \n",
    "if REBUILD_DATA:\n",
    "    dogsvscats = DogsVSCats()\n",
    "    dogsvscats.make_training_data()\n",
    "                    \n",
    "training_data = np.load(\"training_data.npy\",allow_pickle=True)\n",
    "\n",
    "print(len(training_data))\n",
    "\n",
    "class Net(nn.Module) :\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,32,5)   #5= kernel size\n",
    "        self.conv2 = nn.Conv2d(32,64,5)\n",
    "        self.conv3 = nn.Conv2d(64,128,5)\n",
    "        \n",
    "        x=torch.randn(50,50).view(-1,1,50,50)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "        self.fc1 = nn.Linear(self._to_linear,512)\n",
    "        self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "    def convs(self, x):\n",
    "        x=F.max_pool2d(F.relu(self.conv1(x)),(2,2))\n",
    "        x=F.max_pool2d(F.relu(self.conv2(x)),(2,2))\n",
    "        x=F.max_pool2d(F.relu(self.conv3(x)),(2,2))\n",
    "\n",
    "        print(x[0].shape)\n",
    "\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear= x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)\n",
    "         # so we reshape to be flattened as when it comes out of the convs  it is not flat yet\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "net=Net()\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.001)\n",
    "loss_function=nn.MSELoss()\n",
    "\n",
    "x=torch.Tensor([i[0] for i in training_data]).view(-1,50,50)\n",
    "x=x/255\n",
    "y=torch.Tensor([i[1] for i in training_data])\n",
    "\n",
    "VAL_PCT = 0.1\n",
    "val_size = int(len(x)*VAL_PCT)\n",
    "print(val_size)\n",
    "\n",
    "train_x = x[:-val_size]\n",
    "train_y = y[:-val_size]\n",
    "test_x = x[-val_size:]\n",
    "test_y = y[-val_size:]\n",
    "print(len(train_x))\n",
    "print(len(test_x))\n",
    "\n",
    "Batch_size = 100\n",
    "EPOCHS = 2\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     for i in tqdm(range(0,len(train_x),Batch_size)):\n",
    "#         # print(i,i+Batch_size)\n",
    "#         # we are going to start from 0 till len(train_x and we will take the stpes of BATCH_SIZE )\n",
    "#         batch_x = train_x[i:i+Batch_size].view(-1,1,50,50)\n",
    "#         batch_y = train_y[i:i+Batch_size]\n",
    "#         # fitment optimization - zero the gradients\n",
    "        \n",
    "#         net.zero_grad()\n",
    "#         outputs = net(batch_x)\n",
    "#         loss=loss_function(outputs, batch_y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# print(loss)\n",
    "\n",
    "# # testing\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# with torch.no_grad():\n",
    "#     for i in tqdm(range(len(test_x))):\n",
    "#         real_class = torch.argmax(test_y[i])\n",
    "#         net_out = net(test_x[i].view(-1,1,50,50))[0]\n",
    "#         predicted_class = torch.argmax(net_out)\n",
    "#         if predicted_class == real_class:\n",
    "#             correct+=1\n",
    "#         total+=1\n",
    "\n",
    "# print('ACCURACY = ', round(correct/total,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "001a06e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x,y, train=False):\n",
    "    # when we pass data through here by default we will not update weights\n",
    "    if train:\n",
    "        net.zero_grads()\n",
    "    outputs=net(x)\n",
    "    # regardless of the fact that data is in sample or out of sample we are going to calculate accuracy and we will compare two\n",
    "    # beacuse most of the time in sample accuracy will be greater than out of sample accuracy\n",
    "    matches = [torch.argmax(i) == torch.argmax(j) for i, j in zip(outputs, y)]\n",
    "    acc = matches.count(True)/len(matches)\n",
    "    loss=loss_function(outputs, y)\n",
    "\n",
    "    if train:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return acc,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "188ae5fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m         val_acc, val_loss \u001b[38;5;241m=\u001b[39m forward_pass(x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m50\u001b[39m,\u001b[38;5;241m50\u001b[39m), y)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m val_acc, val_loss\n\u001b[1;32m----> 8\u001b[0m val_acc, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(val_acc, val_loss)\n",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(size)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest\u001b[39m(size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     random_start\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_x\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m)\n\u001b[0;32m      3\u001b[0m     x,y \u001b[38;5;241m=\u001b[39m test_x[random_start:random_start\u001b[38;5;241m+\u001b[39msize],test_y[random_start:random_start\u001b[38;5;241m+\u001b[39msize]\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "def test(size=32):\n",
    "    random_start=np.random.randint(len(test_x).size)\n",
    "    x,y = test_x[random_start:random_start+size],test_y[random_start:random_start+size]\n",
    "    with torch.no_grad():\n",
    "        val_acc, val_loss = forward_pass(x.view(-1,1,50,50), y)\n",
    "    return val_acc, val_loss\n",
    "\n",
    "val_acc, val_loss = test(size=100)\n",
    "print(val_acc, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f41dd0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720ab3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"model - {int(time.time())}\"  # everytime you run a model or make a new model it will have a new time so the data will not crash with each other and the model will be saved with diff name\n",
    "net=Net()\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.001)\n",
    "loss_function=nn.MSELoss()\n",
    "\n",
    "print(model_name)\n",
    "\n",
    "def train():\n",
    "    Batch_size = 100\n",
    "    EPOCHS = 5\n",
    "    with open(\"model.log\",\"a\") as f:\n",
    "        for epoch in range(EPOCHS):\n",
    "            for i in tqdm(range(0,len(train_x),Batch_size)):\n",
    "                batch_x = train_x[i:i+Batch_size].view(-1,1,50,50)\n",
    "                batch_y = train_y[i:i+Batch_size]\n",
    "\n",
    "                acc, loss = forward_pass(batch_x,batch_y, train=True)\n",
    "                if i%50 == 0\n",
    "                val_acc, val_loss = forward_pass(size=100)\n",
    "                # save it to that file\n",
    "                f.write(f\"{model_name}, {round(time.time(),3)},{round(float(acc),2)},{round(float(loss),4)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
